
# Homework assignment 1
## A personalization case study, and optimization & regularization
### Shuo Yang (sy2886) sy2886@columbia.edu

## Assignment

For this exercise we will try to predict the subjective quality of Portuguese red wine using only the physiochemical properties of each wine. We will not use any the common regression methods and packages for Python or R - instead we will use minimization methods in order to minimize the error of a linear model. In this case, we will define error as Residual Sum of Squares (RSS), which is also often known as Squared Error.
Though the instructions below are specific to Python, the deliverable for this assignment can be either a Python or R notebook (markdown or pdf). Submit only this notebook (all code and question answers should be in this one file). This exercise should be relatively straight forward, so don’t overthink it, and keep it short and clear. Include the following steps.

## Setup

- Download and load the data set for red wine into a data frame 
  - http://archive.ics.uci.edu/ml/datasets/Wine+Quality
  - Use only the red wine data, not the white wine data
- Split the data by columns into features that you will use for prediction, X, and the feature you will try to predict (‘quality’), y
- Split both X and y by rows into training sets a testing sets
  - Randomly split the data, keeping 80% of instances for training and 20% for testing
- At the end, you should have 4 data sets: X_train, y_train, X_test, and y_test



```python
import pandas as pd
import numpy as np

df = pd.read_csv('winequalityred.csv',sep=";",header=0)
df.columns = [c.replace(' ','_') for c in df.columns]
df.head();
```


```python
from sklearn.model_selection import train_test_split
X = df.iloc[:,0:-1]
y = df.iloc[:,-1:]

X_train, X_test, y_train, y_test = train_test_split(
     X, y, test_size=0.2, random_state=123)
```


## Regression equations and functions

- Write out two equations: 
  (1) the equation for a the linear model that predicts y from X, and 
  (2) the equation for computing the Residual Sum of Squares (RSS) for the linear model, given data, vector x, and parameters, vector $\beta$.
  - See equations 3.1 and 3.2 in the Elements of Statistical Learning book
  - Feel free to ignore the intercept term for this homework (e.g. $\beta_0$)
- Translate these equations into code in the form of two functions
  - The first function should compute the estimated value of y, which is y_hat, for particular values of x, and $\beta$. That is, there should be two arguments, one for the data and one for the linear function parameters.
  - The second function should compute the RSS for the first function

1.  Equations
$X = (X_1, X_2, ... X_p)$, our training set in this project is of dimention $(N, p) = (1599, 11)$. $\beta_0$ is ignored.

    1. Linear model to predic y from X
 $$f(X) = \sum_{j=i}^{p} X_j\beta_j$$
    1. Computing RSS
 $$RSS(\beta) = \sum_{j=i}^{N} (y_i - \sum_{j=1}^{p} x_{ij}\beta_j)^2$$


```python
def y_hat (beta, X):
  y = X.dot(beta)
  y = pd.DataFrame({"quality":y})
  return y

def rss (beta, X, y):
  y_hat_v = y_hat(beta, X)
  rss = y.sub(y_hat_v).pow(2)
  return rss.sum().iloc[0]
```

## Optimizing the model
- Use Scipy’s minimize function to find the value of $\beta$ that minimize the RSS
  - https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html
  - Your call to minimize method will take three arguments:
     - (1) fun: the RSS function you defined above that you are trying to minimize
     - (2) $x_0$: your initial values of $\beta$
     - (3) args: pass in all the data a tuple here. For example:
        - args=(y_train, X_train)
  - For the second argument you will need to initialize $\beta$ to some starting value. Try using a random vector with Numpy random methods
    - numpy.random.normal(0, 1, X_train.shape[1])
  - Your final set of functions to fit your model should have the form:
           def RSS(beta, X, y):
                    return <some_results>
           res = minimize(fun=RSS, x0=beta0, args=(X_train,y_train))
           beta_hat = res.x



```python
import scipy.optimize as opt
np.random.seed(123)
beta0 = np.random.normal(0,1,(X_train.shape[1]))
res = opt.minimize(fun=rss,x0=beta0,args=(X_train, y_train))
```


```python
beta_hat= res.x
res;
rss(res.x,X_train,y_train), rss(res.x,X_test,y_test)
```




    (529.2778573540102, 139.60914628917226)




```python
pd.DataFrame(beta_hat).transpose().set_axis(X_train.columns,axis="columns",inplace=False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed_acidity</th>
      <th>volatile_acidity</th>
      <th>citric_acid</th>
      <th>residual_sugar</th>
      <th>chlorides</th>
      <th>free_sulfur_dioxide</th>
      <th>total_sulfur_dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.011716</td>
      <td>-1.126652</td>
      <td>-0.088428</td>
      <td>0.004734</td>
      <td>-1.995074</td>
      <td>0.002191</td>
      <td>-0.00328</td>
      <td>4.651854</td>
      <td>-0.464755</td>
      <td>0.824085</td>
      <td>0.286963</td>
    </tr>
  </tbody>
</table>
</div>



### Questions:
- What are the qualitative results from your model? Which features seem to be most
important? Do you think that the magnitude of the features in X may affect the results (for example, the average total sulfur dioxide across all wines is 46.47, but the average chlorides is only 0.087).



```python
#summary table of correlation with quality, mean and std for each feature in original dataset
summary = pd.DataFrame(np.vstack((df.corr().iloc[-1,:],df.mean(),df.std() ))).set_axis(df.columns, axis='columns', inplace=False)
summary = summary.set_axis({'corr','mean','std'},axis="index",inplace=False)
```


```python
#summary table of correlation with quality, mean and std for each feature in X_train dataset
df_train = X_train.join(y_train)
summary_train = pd.DataFrame(np.vstack((df_train.corr().iloc[-1,:],df_train.mean(),df_train.std() ))).set_axis(df_train.columns, axis='columns', inplace=False)
summary_train = summary_train.set_axis({'corr','mean','std'},axis="index",inplace=False)
```


```python
summary_train
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed_acidity</th>
      <th>volatile_acidity</th>
      <th>citric_acid</th>
      <th>residual_sugar</th>
      <th>chlorides</th>
      <th>free_sulfur_dioxide</th>
      <th>total_sulfur_dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>std</th>
      <td>0.104519</td>
      <td>-0.399479</td>
      <td>0.221081</td>
      <td>0.002350</td>
      <td>-0.137099</td>
      <td>-0.072373</td>
      <td>-0.201756</td>
      <td>-0.186942</td>
      <td>-0.035631</td>
      <td>0.251701</td>
      <td>0.479006</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>8.331900</td>
      <td>0.524808</td>
      <td>0.272213</td>
      <td>2.556685</td>
      <td>0.087625</td>
      <td>16.384285</td>
      <td>47.864738</td>
      <td>0.996769</td>
      <td>3.309945</td>
      <td>0.660039</td>
      <td>10.405655</td>
      <td>5.631744</td>
    </tr>
    <tr>
      <th>corr</th>
      <td>1.755532</td>
      <td>0.179812</td>
      <td>0.194180</td>
      <td>1.445628</td>
      <td>0.048450</td>
      <td>10.809485</td>
      <td>33.175107</td>
      <td>0.001895</td>
      <td>0.154436</td>
      <td>0.172395</td>
      <td>1.066217</td>
      <td>0.805668</td>
    </tr>
  </tbody>
</table>
</div>




```python
summary
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed_acidity</th>
      <th>volatile_acidity</th>
      <th>citric_acid</th>
      <th>residual_sugar</th>
      <th>chlorides</th>
      <th>free_sulfur_dioxide</th>
      <th>total_sulfur_dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>std</th>
      <td>0.124052</td>
      <td>-0.390558</td>
      <td>0.226373</td>
      <td>0.013732</td>
      <td>-0.128907</td>
      <td>-0.050656</td>
      <td>-0.185100</td>
      <td>-0.174919</td>
      <td>-0.057731</td>
      <td>0.251397</td>
      <td>0.476166</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>8.319637</td>
      <td>0.527821</td>
      <td>0.270976</td>
      <td>2.538806</td>
      <td>0.087467</td>
      <td>15.874922</td>
      <td>46.467792</td>
      <td>0.996747</td>
      <td>3.311113</td>
      <td>0.658149</td>
      <td>10.422983</td>
      <td>5.636023</td>
    </tr>
    <tr>
      <th>corr</th>
      <td>1.741096</td>
      <td>0.179060</td>
      <td>0.194801</td>
      <td>1.409928</td>
      <td>0.047065</td>
      <td>10.460157</td>
      <td>32.895324</td>
      <td>0.001887</td>
      <td>0.154386</td>
      <td>0.169507</td>
      <td>1.065668</td>
      <td>0.807569</td>
    </tr>
  </tbody>
</table>
</div>




  1. The quality prediction is positively correlated with 5 features: residual_sugar, free_sulfur_dioxide, density, sulphates and alcohol, and is negatively correlated with the rest 6 features fixed_acidity, volatile_acidity, citric_acid, chlorides, total_sulfur_dioxide and ph.
  1. Density, chlorides and volatile seem to be the most important features, as absolute values of their beta_hat are the largest. Density is positive with quality, while chlorides and volatile are negative correlated with quality. However, this may not indicate the true importance. The coefficients coordinate the information of magnitude of features. For example, chlorides is averaged around 0.087. Although its coefficient is the second highest in magnitude, the information it contributes in determing the quality is still relatively small comparing to the others.
  1. Density is the feature with the least standard deviation of 0.001887 among all the features. But its correlation with quality is negative and not very high. Since our model here ignores the intercept term, the density feature is kind of coordinated for the intercept term. Thus density itself should not be that informative in predicting quality of the wine.
  1. The magnitude of the features in X may affect the estimation of $\beta$, but will not affect the results of RSS. We will discuss this by rescaling total_sulfur_dioxide (divided by 10) and chlorides (multiplied by 10), which are the maximum and minimum average magnitude in these features.


```python
##change magnitude of total_sulfur_dioxide and chlorides
nl_X_train = X_train.assign (chlorides=X_train.chlorides*10,
                             total_sulfur_dioxide=X_train.total_sulfur_dioxide/10)

nl_X_test = X_test.assign (chlorides=X_test.chlorides*10,
                           total_sulfur_dioxide=X_test.total_sulfur_dioxide/10)
```


```python
nl_res = opt.minimize(fun=rss,x0=beta0,args=(nl_X_train, y_train))
nl_beta_hat= nl_res.x
rss(nl_res.x,nl_X_train,y_train), rss(nl_res.x,nl_X_test,y_test)
```




    (529.2778573518453, 139.6091442659816)




```python
pd.DataFrame(nl_beta_hat).transpose().set_axis(nl_X_train.columns,axis="columns",inplace=False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed_acidity</th>
      <th>volatile_acidity</th>
      <th>citric_acid</th>
      <th>residual_sugar</th>
      <th>chlorides</th>
      <th>free_sulfur_dioxide</th>
      <th>total_sulfur_dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.011716</td>
      <td>-1.126653</td>
      <td>-0.08843</td>
      <td>0.004734</td>
      <td>-0.199507</td>
      <td>0.002191</td>
      <td>-0.032795</td>
      <td>4.651842</td>
      <td>-0.464753</td>
      <td>0.824085</td>
      <td>0.286963</td>
    </tr>
  </tbody>
</table>
</div>



1. The beta_hat is only changed for total_sulfur_dioxide (10 times the original coefficient) and chlorides (1/10 of the original coefficient, while the others unchanged. The RSS for both training and testing data kept unchanged, as the optimal coefficients are asjusted for the feature magnitude.

- How well does your model fit? You should be able to measure the goodness of fit, RSS, on both the training data and the test data, but only report the results on the test data. In Machine Learning we almost always only care about how well the model fits on data that has not been used to fit the model, because we need to use the model in the future, not the past. Therefore, we only report performance with holdout data, or test data.



```python
import matplotlib.pyplot as pl
y_pre_train = y_hat(beta_hat,X_train)
resi_train = y_train-y_pre_train
resi_std_train = (resi_train-resi_train.mean())/resi_train.std()
pl.plot(y_pre_train,resi_std_train,'o')
pl.hlines(0,y_pre_train.min(),y_pre_train.max())
```




    <matplotlib.collections.LineCollection at 0x1a1e52e438>




![png](output_23_1.png)



```python
y_pre_test = y_hat(beta_hat,X_test)
resi_test = y_test-y_pre_test
resi_std_test = (resi_test-resi_test.mean())/resi_test.std()
pl.plot(y_pre_test,resi_std_test,'o')
pl.hlines(0,y_pre_test.min(),y_pre_test.max())
```




    <matplotlib.collections.LineCollection at 0x1a1e607a90>




![png](output_24_1.png)


1. RSS_train is 529.2778573518453 while RSS_test is 139.6091442659816. If we take set size into consideration, RSS_train is slightly smaller than the RSS_test on average. This is intuitive as the model is built on training set.
2. Standardized resisual is plotted against the predicted y for both training set (first plot) and test set (second plot). The training set residual plot looks fine. The test set residual plot shows that the model might not be a good fit. The residual is not randomly spread out among the 0 line, and it seems to be a lot underestimate for the higher y values.

- Does the end result or RSS change if you try different initial values of $\beta$? What happens if you change the magnitude of the initial $\beta$?


```python
#beta0 changes random seed
np.random.seed(222)
beta0 = np.random.normal(0,1,(X_train.shape[1]))
res = opt.minimize(fun=rss,x0=beta0,args=(X_train, y_train))
beta_hat= res.x
rss(res.x,X_train,y_train), rss(res.x,X_test,y_test)
```




    (529.2778573534741, 139.60914129334066)




```python
#beta0 changes magnitude
beta0 = np.random.normal(1,100,(X_train.shape[1]))
res = opt.minimize(fun=rss,x0=beta0,args=(X_train, y_train))
beta_hat= res.x
rss(res.x,X_train,y_train), rss(res.x,X_test,y_test)
```




    (529.2778573537557, 139.60914383705534)



1. Changing initial values of $\beta$ does not change RSS very much. Normally they are the same upto 5 digits after the decimal place for a few tried different initial values.

- Does the choice of solver method change the end result or RSS?


```python
#method: Powell
np.random.seed(123)
beta0 = np.random.normal(0,1,(X_train.shape[1]))
res = opt.minimize(fun=rss,x0=beta0,args=(X_train, y_train),method='Powell')
beta_hat= res.x
rss(res.x,X_train,y_train), rss(res.x,X_test,y_test)
```




    (529.2813068760752, 139.6921256269703)




```python
#method: Nelder-Mead
np.random.seed(123)
beta0 = np.random.normal(0,1,(X_train.shape[1]))
res = opt.minimize(fun=rss,x0=beta0,args=(X_train, y_train),method='Nelder-Mead')
beta_hat= res.x
rss(res.x,X_train,y_train), rss(res.x,X_test,y_test)
```




    (650.7802592479632, 158.8068019112638)



1. Yes. Two other methods are tried other than the default method 'BFGS'. The results of RSS are different

## Regularizing the model
Regularization seeks to simplify a model by decreasing the model’s complexity and degrees of freedom. While lowering the degrees of freedom also decreases the flexibility of the model, and therefore the performance of the model on training data, it increases generalizability, and thus it often increases performance on test data. One common method of regularization is called shrinkage, and is defined in section 3.4 of Elements of Statistical Learning.
- Try adding in an L2 (aka Ridge) regularization penalty to your model above to create a new, regularized model. See equation 3.41 for guidance. You will need to choose a value of lambda, so start with something small, like 0.01.
- How does RSS on the training data change? How does RSS on the test data change?
- What happens if you try different values of lambda? Can you tune lambda to get the best
results on the test data?
- Now try using an L1 (aka Lasso) regularization penalty instead. See equation 3.51 for
example. Report your findings on how RSS changes, and if you can roughly tune lambda.
- Again, do you think that the magnitude of the features in X may affect the results with
regularization?

### L2 (aka Ridge) Regularized Model


```python
# L2 (aka Ridge)
def ridge_rss(beta,lamda,X, y):
  r = rss(beta, X, y)
  r_s = r + lamda * sum(beta ** 2)
  return r_s
```


```python
np.random.seed(123)
beta0 = np.random.normal(0,1,(X_train.shape[1]))
ridge_res = opt.minimize(fun=ridge_rss,x0=beta0,args=(0.01,X_train, y_train))
ridge_beta_hat = ridge_res.x
rss(ridge_beta_hat,X_train,y_train), rss(ridge_beta_hat,X_test,y_test)
```




    (529.2809460719947, 139.59246735261553)




```python
pd.DataFrame(ridge_beta_hat).transpose().set_axis(X_train.columns,axis="columns",inplace=False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed_acidity</th>
      <th>volatile_acidity</th>
      <th>citric_acid</th>
      <th>residual_sugar</th>
      <th>chlorides</th>
      <th>free_sulfur_dioxide</th>
      <th>total_sulfur_dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.010681</td>
      <td>-1.127046</td>
      <td>-0.089854</td>
      <td>0.004721</td>
      <td>-1.975395</td>
      <td>0.002176</td>
      <td>-0.003262</td>
      <td>4.594302</td>
      <td>-0.451335</td>
      <td>0.823289</td>
      <td>0.287266</td>
    </tr>
  </tbody>
</table>
</div>



1. Comparing to the original model, the RSS for the training data increases while the RSS for the testing data decreases, with lambda of 0.01. 


```python
#Find optimal lambda
out_ridge = pd.DataFrame(columns=['lamda', 'RSS_train', 'RSS_test'])

for i in np.arange(0.01,0.5,0.01):
    r_res = opt.minimize(fun=ridge_rss,x0=beta0,args=(i,X_train, y_train))
    out_ridge = out_ridge.append(pd.Series([i,rss(r_res.x,X_train,y_train), rss(r_res.x,X_test,y_test)], index=out_ridge.columns ), ignore_index=True)    
```


```python
out_ridge;
```


```python
pl.plot(out_ridge['lamda'],out_ridge['RSS_test'],'o')
```




    [<matplotlib.lines.Line2D at 0x1a1e7010b8>]




![png](output_42_1.png)



```python
out_ridge.loc[out_ridge['RSS_test'].idxmin()]
```




    lamda          0.160000
    RSS_train    529.841080
    RSS_test     139.495342
    Name: 15, dtype: float64




```python
pl.plot(out_ridge['lamda'],out_ridge['RSS_train'],'o');
```


![png](output_44_0.png)



```python
out_ridge.loc[out_ridge['RSS_train'].idxmin()]
```




    lamda          0.010000
    RSS_train    529.280946
    RSS_test     139.592467
    Name: 0, dtype: float64



1. We first checked RSS for lambdas from 0.01 to 0.5 by 0.01. The first plot shows RSS_test changes when lambda changes. RSS_test decreases at the beginning when lambda increases, and then goes up with lambda. The optimal lambda is **0.16** to achieve the best RSS_test **139.592467**. 
2. The second plot is for RSS_train, which is always increasing with lambda. This is intuitive from the formulation of the regularization penalty. 
3. Larger choices of lambda are also tried, but the RSS_test gets larger as lambda increases. We will skip the details.

### L1 (aka Lasso) Regularized Model


```python
# L1 (aka Lasso)
def lasso_rss(beta, lamda, X, y):
    r = rss(beta, X, y)
    r_l = 0.5 * r + lamda * sum(abs(beta))
    return r_l
```


```python
lasso_res = opt.minimize(fun=lasso_rss,x0=beta0,args=(0.01,X_train, y_train))
lasso_beta_hat = lasso_res.x
rss(lasso_res.x,X_train,y_train), rss(lasso_res.x,X_test,y_test)
```




    (529.2781406077617, 139.60861442530347)




```python
#find optimal lambda
out_lasso = pd.DataFrame(columns=['lamda', 'RSS_train', 'RSS_test'])

for i in np.arange(0.01,0.5,0.01):
    r_res = opt.minimize(fun=lasso_rss,x0=beta0,args=(i,X_train, y_train))
    out_lasso = out_lasso.append(pd.Series([i,rss(r_res.x,X_train,y_train), rss(r_res.x,X_test,y_test)], index=out_lasso.columns ), ignore_index=True)   
```

    /Users/ys/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py:1046: RuntimeWarning: divide by zero encountered in double_scalars
      rhok = 1.0 / (numpy.dot(yk, sk))



```python
out_lasso;
```


```python
pl.plot(out_lasso['lamda'],out_lasso['RSS_test'],'o')
```




    [<matplotlib.lines.Line2D at 0x1a1e8659b0>]




![png](output_52_1.png)



```python
out_lasso.loc[out_lasso['RSS_test'].idxmin()]
```




    lamda          0.050000
    RSS_train    529.284943
    RSS_test     139.607728
    Name: 4, dtype: float64




```python
pl.plot(out_lasso['lamda'],out_lasso['RSS_train'],'o')
```




    [<matplotlib.lines.Line2D at 0x1a1e9278d0>]




![png](output_54_1.png)



```python
out_lasso.loc[out_lasso['RSS_train'].idxmin()]
```




    lamda          0.010000
    RSS_train    529.278141
    RSS_test     139.608614
    Name: 0, dtype: float64



1. We first try for lambdas of 0.01. lasso_RSS_test is 139.60861442530347, which is smaller than original RSS_test 139.60914628917226. The training RSS is till larger than original.
2. We then go for lambdas from 0.01 to 0.5 by 0.01 as we tried in Ridge model. The first plot shows the distribution of RSS_test when lambda changes. The optimal lambda is **0.05** to achieve the best RSS_test **139.607728**, which is smaller than it for the original model. The trend in the beginning is not that clear to observe as the differences of RSS_test are quite small. When we zoom in to the (0.01, 0.1) range, the RSS_test is indeed decreasing before lambda reaches 0.05 and increasing afterwards.
3. The second plot is for RSS_train, which is increasing with lambda. The optimal RSS_train is achieved at the smallest value of lambda (0.01) within our selected range. The RSS_train is greater than what we have for original model.

### Magnitude of the Features in X may Affect the Results with Regularization


```python
# L2 (aka Ridge)
nl_res = opt.minimize(fun=ridge_rss,x0=beta0,args=(0.16,nl_X_train, y_train))
nl_beta_hat= nl_res.x
rss(nl_res.x,nl_X_train,y_train), rss(nl_res.x,nl_X_test,y_test)
```




    (529.7427699414911, 139.5309671948269)




```python
# L1 (aka Lasso)
nl_res = opt.minimize(fun=lasso_rss,x0=beta0,args=(0.05,nl_X_train, y_train))
nl_beta_hat= nl_res.x
rss(nl_res.x,nl_X_train,y_train), rss(nl_res.x,nl_X_test,y_test)
```




    (529.2827664692726, 139.6167628059951)



1. The magnitude of the features in X may affect the results with regularization, as $\beta$ will coordinate the magnitude of the features and $\beta$ is added as regularization penalty. 
2. Here we have examples using previous optimal lambda for each model and the X which twists the magnitude of total_sulfur_dioxide and chlorides. For Ridge model, RSS_test is 139.5309671948269, while the previous optimal RSS_test is 139.59246735261553. For Lasso model, RSS_test is 139.6167628059951, while the previous optimal RSS_test is 139.607728. Both results change.
